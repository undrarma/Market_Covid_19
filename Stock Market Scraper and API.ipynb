{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analytics - Assignment 2 - Andrea Armani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are scraped from Wikipedia website at https://en.wikipedia.org/wiki/List_of_S%26P_500_companies .\n",
    "According to the Terms and Conditions applied to such website, scraping data for academic purposes is allowed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covid pandemic massively affected the financial stock markets around the world. In the following, I collected about the fluctuations of the **S&P 500** companies, the most important companies in the american stock market. For the sake of simplicity such collection is limited to the US market, however using the same code is possible to retrieve data about companies listed in different stock markets around the world. \n",
    "Other lists that contain the most important companies in different world's region could be find at:<br>\n",
    "<br>\n",
    "https://en.wikipedia.org/wiki/S%26P/ASX_50 (Australia)<br>\n",
    "https://en.wikipedia.org/wiki/S%26P_Asia_50 (Asia)<br>\n",
    "https://en.wikipedia.org/wiki/S%26P/TSX_60 (Canada)<br>\n",
    "https://en.wikipedia.org/wiki/S%26P_Latin_America_40 (South America)<br>\n",
    "<br>\n",
    "As for the european markets the S&P listed 350 companies, however there is no available scraping list from which data can be retrieved without infringe Terms and Conditions of websites. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import os.path\n",
    "from os import path\n",
    "import requests\n",
    "import unidecode\n",
    "import yfinance as yf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data Folder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this folder there will be the Wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_scraped = \"scraped_companies_data\"\n",
    "\n",
    "if not path.isdir(directory_scraped):\n",
    "    os.mkdir(directory_scraped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Data Folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This folder will contain the data retrieved through API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_api_files = \"api_history_sep_companies\"\n",
    "\n",
    "if not path.isdir(directory_api_files):\n",
    "    os.mkdir(directory_api_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve an S&P pages in wikipedia\n",
    "def fetch_sep_lists(root,page_name,directory):\n",
    "    full_url = root + page_name\n",
    "    \n",
    "    true_name = page_name.replace(\"/\",'_')\n",
    "    #the  name of the file to save to\n",
    "    file_name = directory+\"/\"+true_name+\".html\"\n",
    "    \n",
    "    \n",
    "    page = requests.get(full_url, allow_redirects=True)\n",
    "    \n",
    "    \n",
    "    if not page.status_code == 200:\n",
    "        return \"Some errors occured!\"\n",
    "    else:\n",
    "        open(file_name, 'wb').write(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It accesses the scraped page and returns a list of tickers.<br>\n",
    "Such stickers can be find in the table 'constituents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ticker_sep_america(file_name):\n",
    "    \n",
    "    tickers = []\n",
    "    \n",
    "    with open(file_name, encoding='utf-8') as fp:\n",
    "        soup = bs(fp, 'html.parser')\n",
    "   \n",
    "    # Select the table with id = constituents\n",
    "    table = soup.find(id = \"constituents\")\n",
    "    \n",
    "    # Within the tbody\n",
    "    tbody = table.find('tbody')\n",
    "    \n",
    "    # Collect the link of every company\n",
    "    rows = tbody.findAll('tr')\n",
    "    rows.pop(0)\n",
    "    for row in rows:\n",
    "        #print(row)\n",
    "        td = row.find('td')\n",
    "        tickers.append(unidecode.unidecode(td.text.strip()))\n",
    "    \n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fetch_sep_lists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-0f51e2e46dbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mroot_page\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://en.wikipedia.org/wiki/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mamerican_sep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfetch_sep_lists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_page\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'List_of_S%26P_500_companies'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdirectory_scraped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'fetch_sep_lists' is not defined"
     ]
    }
   ],
   "source": [
    "# Fetch the S&P page in Wikipedia that contains a list of companies in the Stock Market\n",
    "root_page = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "american_sep = fetch_sep_lists(root_page,'List_of_S%26P_500_companies',directory_scraped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = extract_ticker_sep_america(directory_scraped + '/' + 'List_of_S%26P_500_companies' + '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.DataFrame(tickers, columns=['id'])\n",
    "d.to_csv('american_tickers.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After further analysis, yfinance is not capable to handle tickers that contains special characters. Although it's not the case for the american S&P, I tested the API with tickers from different regions that present such problem and it returned an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tickers =[]\n",
    "for t in tickers:\n",
    "    if t.isalpha():\n",
    "        final_tickers.append(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stock Market Historical Data Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every ticker, it retrieves information about the floating of the market in every specific day from **2019-09-01** up to **2020-09-07**, along with the sector in which the company operates. Finally, it stores such informations in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in range(208,len(final_tickers)):\n",
    "    t = yf.Ticker(final_tickers[ticker])\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    if 'sector' in t.info:\n",
    "        t_historical['sector'] = t.info['sector']\n",
    "    else:\n",
    "        t_historical['sector'] = ''\n",
    "    name = t.info['longName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    t_historical.to_csv(directory_api_files + '/' + \"2019-09-01_2020-09-07_\" + alphanumeric + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in os.listdir(directory_api_files):\n",
    "    x = pd.read_csv(directory_api_files + '/' + filename)\n",
    "    x['Avg'] = (x['Low'] + x['High'])/2\n",
    "    x.to_csv(directory_api_files + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('china indexes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_api_files = \"api_history_sep_companies_china\"\n",
    "\n",
    "if not path.isdir(directory_api_files):\n",
    "    os.mkdir(directory_api_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in l:\n",
    "    t = yf.Ticker(ticker)\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    if 'sector' in t.info:\n",
    "        t_historical['sector'] = t.info['sector']\n",
    "    else:\n",
    "        t_historical['sector'] = ''\n",
    "    t_historical['country'] = t.info['country']\n",
    "    name = t.info['longName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    for i,row in t_historical.iterrows():\n",
    "        if row['country'] == 'China' or row['country'] == 'Macau':\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/7.75\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/7.75\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/7.75\n",
    "        elif row['country'] == 'Taiwan':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/28.62\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/28.62\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/28.62\n",
    "        elif row['country'] == 'Singapore':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/1.36\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/1.36\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/1.36\n",
    "        elif row['country'] == 'South Korea':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/148.22\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/148.22\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/148.22\n",
    "    t_historical.to_csv(directory_api_files + '/' + \"2019-09-01_2020-09-07_\" + alphanumeric + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in os.listdir(directory_api_files):\n",
    "    x = pd.read_csv(directory_api_files + '/' + filename)\n",
    "    x['Avg'] = (x['Low'] + x['High'])/2\n",
    "    x.to_csv(directory_api_files + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('european indexes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_api_files = \"api_history_sep_companies_europe\"\n",
    "\n",
    "if not path.isdir(directory_api_files):\n",
    "    os.mkdir(directory_api_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in l:\n",
    "    t = yf.Ticker(ticker)\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    if 'sector' in t.info:\n",
    "        t_historical['sector'] = t.info['sector']\n",
    "    else:\n",
    "        t_historical['sector'] = ''\n",
    "    t_historical['country'] = t.info['country']\n",
    "    name = t.info['longName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    for i,row in t_historical.iterrows():\n",
    "        t_historical.at[i,'Close'] = t_historical.at[i,'Close']/0.85\n",
    "        t_historical.at[i,'High'] = t_historical.at[i,'High']/0.85\n",
    "        t_historical.at[i,'Open'] = t_historical.at[i,'Open']/0.85\n",
    "    t_historical.to_csv(directory_api_files + '/' + \"2019-09-01_2020-09-07_\" + alphanumeric + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for filename in os.listdir(directory_api_files):\n",
    "    x = pd.read_csv(directory_api_files + '/' + filename)\n",
    "    x['Avg'] = (x['Low'] + x['High'])/2\n",
    "    x.to_csv(directory_api_files + '/' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_api_files = \"api_history_sep_companies_overall_indices\"\n",
    "\n",
    "if not path.isdir(directory_api_files):\n",
    "    os.mkdir(directory_api_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in l:\n",
    "    t = yf.Ticker(ticker)\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    name = t.info['shortName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    t_historical.to_csv(directory_api_files + '/' + \"2019-09-01_2020-09-07_\" + alphanumeric + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged Europe + China + USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder creation\n",
    "directory_api_files = \"api_history_sep_companies_merged_indexes\"\n",
    "\n",
    "if not path.isdir(directory_api_files):\n",
    "    os.mkdir(directory_api_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('european indexes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Europe, China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in l:\n",
    "    t = yf.Ticker(ticker)\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    t_historical.drop(columns = ['Dividends','Stock Splits'],inplace = True)\n",
    "    if 'sector' in t.info:\n",
    "        t_historical['Sector'] = t.info['sector']\n",
    "    else:\n",
    "        t_historical['Sector'] = ''\n",
    "    t_historical['Country'] = t.info['country']\n",
    "    t_historical['Company'] = t.info['longName']\n",
    "    name = t.info['longName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    for i,row in t_historical.iterrows():\n",
    "        if row['Country'] == 'China' or row['Country'] == 'Macau':\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/7.75\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/7.75\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/7.75\n",
    "        elif row['Country'] == 'Taiwan':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/28.62\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/28.62\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/28.62\n",
    "        elif row['Country'] == 'Singapore':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/1.36\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/1.36\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/1.36\n",
    "        elif row['Country'] == 'South Korea':\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/148.22\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/148.22\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/148.22\n",
    "        else:\n",
    "            t_historical.at[i,'Close'] = t_historical.at[i,'Close']/0.85\n",
    "            t_historical.at[i,'High'] = t_historical.at[i,'High']/0.85\n",
    "            t_historical.at[i,'Open'] = t_historical.at[i,'Open']/0.85\n",
    "    df = df.append(t_historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(directory_api_files+'/companies_europe_asia.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('american_tickers.csv')\n",
    "l = list(x.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(directory_api_files+'/companies_europe_asia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in l:\n",
    "    t = yf.Ticker(ticker)\n",
    "    t_historical = t.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "    t_historical.drop(columns = ['Dividends','Stock Splits'],inplace = True)\n",
    "    if 'sector' in t.info:\n",
    "        t_historical['Sector'] = t.info['sector']\n",
    "    else:\n",
    "        t_historical['Sector'] = ''\n",
    "    t_historical['Country'] = t.info['country']\n",
    "    t_historical['Company'] = t.info['longName']\n",
    "    name = t.info['longName']\n",
    "    alphanumeric = ''\n",
    "    if name.isalpha() == False:\n",
    "        for character in name:\n",
    "            if character.isalnum():\n",
    "                alphanumeric += character\n",
    "    df1 = df1.append(t_historical)\n",
    "\n",
    "df1.reset_index(level = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.append(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(directory_api_files + \"/companies_stock_price_worldwide.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['^GSPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = yf.Ticker('^GSPC').history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "a['Index'] = 'S&P 500'\n",
    "\n",
    "\n",
    "a.drop(['Dividends','Stock Splits'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_csv('api_history_sep_companies_overall_indices/Euro Stoxx 50 Historical Data.csv')\n",
    "\n",
    "b['Index'] = 'STOXX 50 EU'\n",
    "\n",
    "b.rename(inplace = True, columns = {'Price':'Close'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = yf.Ticker('^HSI').history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "c['Index'] = 'Hang Sang'\n",
    "\n",
    "c.reset_index(level = 0, inplace = True)\n",
    "\n",
    "c.drop(['Dividends','Stock Splits'],axis = 1,inplace = True)\n",
    "\n",
    "c['High'] = c['High'] * 0.13\n",
    "c['Open'] = c['Open'] * 0.13\n",
    "c['Low'] = c['Low'] * 0.13\n",
    "c['Close'] = c['Close'] * 0.13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in b.iterrows():\n",
    "    a_p =datetime.strptime(r['Date'], '%b %d, %Y')\n",
    "    a_p = a_p.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    vol = r['Vol.']\n",
    "    vol = vol.replace('M','')\n",
    "    vol = vol.replace('-','0')\n",
    "    vol = float(vol)\n",
    "    vol = vol*1.17\n",
    "    vol = int(vol * 1000000)\n",
    "    high = r['High']\n",
    "    high = high.replace(',','')\n",
    "    high = float(high)\n",
    "    high = high*1.17\n",
    "    low = r['Low']\n",
    "    low = low.replace(',','')\n",
    "    low = float(low)\n",
    "    low = low*1.17\n",
    "    opn = r['Open']\n",
    "    opn = opn.replace(',','')\n",
    "    opn = float(opn)\n",
    "    opn = opn*1.17\n",
    "    close = r['Close']\n",
    "    close = close.replace(',','')\n",
    "    close = float(close)\n",
    "    df = pd.DataFrame([[ a_p,opn,high,low,close,vol,r['Index'] ]] ,\n",
    "                      columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Index'])\n",
    "    a = a.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['Avg'] = (a['High'] + a['Low'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('api_history_sep_companies_overall_indices/compounded_indexes.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = pd.DataFrame(columns=['Name','Diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Avg'] = (df_final['High'] + df_final['Low'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_final.loc[df_final['Date'].isin(['2019-09-06','2020-08-31'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = []\n",
    "for i,r in temp.iterrows():\n",
    "    if i == 0:\n",
    "        continue\n",
    "    if temp.iloc[i-1,9] == temp.iloc[i-1,9]:\n",
    "        li.append([temp.iloc[i,10] / temp.iloc[i-1,10],temp.iloc[i,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.sort(reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "avges = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_za = yf.Ticker('ZAL.DE')\n",
    "x_historical = x_za.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "avg_za = (x_historical.High + x_historical.Low)/2\n",
    "\n",
    "x_eb = yf.Ticker('EBAY')\n",
    "x_historical = x_eb.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "avg_eb = (x_historical.High + x_historical.Low)/2\n",
    "\n",
    "x_am = yf.Ticker('AMZN')\n",
    "x_historical = x_am.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "avg_am = (x_historical.High + x_historical.Low)/2\n",
    "\n",
    "x_ba = yf.Ticker('BABA')\n",
    "x_historical = x_ba.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "avg_ba = (x_historical.High + x_historical.Low)/2\n",
    "\n",
    "x_as = yf.Ticker('ASC.L')\n",
    "x_historical = x_as.history(start=\"2019-09-01\", end=\"2020-09-07\", interval=\"1d\")\n",
    "\n",
    "avg_as = (x_historical.High + x_historical.Low)/2\n",
    "\n",
    "avges['Zalando'] = avg_za\n",
    "\n",
    "avges['Ebay'] = avg_eb\n",
    "\n",
    "avges['Asos'] = avg_as\n",
    "\n",
    "avges['Alibaba'] = avg_ba\n",
    "\n",
    "avges['Amazon'] = avg_am\n",
    "\n",
    "normalized_avges=(avges-avges.min())/(avges.max()-avges.min())\n",
    "\n",
    "normalized_avges.reset_index(level = 0, inplace = True)\n",
    "\n",
    "normalized_avges.rename(columns = {'index':'Date'},inplace = True)\n",
    "\n",
    "normalized_avges.fillna(0,inplace = True)\n",
    "\n",
    "normalized_avges.to_csv(directory_api_files + '/' + 'ecommerce_normalized_companies.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
